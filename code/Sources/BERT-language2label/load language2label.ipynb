{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-language2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/notebooks/language2motion.gt/code\")\n",
      "\t\tBatcher\n",
      "\t\tModelSupport\n",
      "\t\tDatasets\n",
      "\t\tTextModels\n",
      "With SwiftPM flags: ['-c', 'release']\n",
      "Working in: /tmp/tmp314zh_vf/swift-install\n",
      "[1/2] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location /notebooks/language2motion.gt/swift-install\n",
    "%install-swiftpm-flags -c release\n",
    "%install '.package(path: \"/notebooks/language2motion.gt/code\")' Batcher ModelSupport Datasets TextModels\n",
    "\n",
    "import Datasets\n",
    "import Foundation\n",
    "import ModelSupport\n",
    "import TensorFlow\n",
    "import TextModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT pre-trained model 'BERT Base Uncased'.\n",
      "Loading resource: uncased_L-12_H-768_A-12\n"
     ]
    }
   ],
   "source": [
    "let bertPretrained = BERT.PreTrainedModel.bertBase(cased: false, multilingual: false)\n",
    "let workspaceURL = URL(\n",
    "    fileURLWithPath: \"bert_models\", isDirectory: true,\n",
    "    relativeTo: URL(\n",
    "        fileURLWithPath: NSTemporaryDirectory(),\n",
    "        isDirectory: true))\n",
    "let bert = try BERT.PreTrainedModel.load(bertPretrained)(from: workspaceURL)\n",
    "var bertClassifier = BERTClassifier(bert: bert, classCount: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset acquired.\n"
     ]
    }
   ],
   "source": [
    "// Regarding the batch size, note that the way batching is performed currently is that we bucket\n",
    "// input sequences based on their length (e.g., first bucket contains sequences of length 1 to 10,\n",
    "// second 11 to 20, etc.). We then keep processing examples in the input data pipeline until a\n",
    "// bucket contains enough sequences to form a batch. The batch size specified in the task\n",
    "// constructor specifies the *total number of tokens in the batch* and not the total number of\n",
    "// sequences. So, if the batch size is set to 1024, the first bucket (i.e., lengths 1 to 10)\n",
    "// will need 1024 / 10 = 102 examples to form a batch (every sentence in the bucket is padded\n",
    "// to the max length of the bucket). This kind of bucketing is common practice with NLP models and\n",
    "// it is done to improve memory usage and computational efficiency when dealing with sequences of\n",
    "// varied lengths. Note that this is not used in the original BERT implementation released by\n",
    "// Google and so the batch size setting here is expected to differ from that one.\n",
    "let maxSequenceLength = 128\n",
    "let batchSize = 1024\n",
    "\n",
    "// Create a function that converts examples to data batches.\n",
    "let exampleMapFn: (CoLA.Example) -> CoLA.DataBatch = { example -> CoLA.DataBatch in\n",
    "    let textBatch = bertClassifier.bert.preprocess(\n",
    "        sequences: [example.sentence],\n",
    "        maxSequenceLength: maxSequenceLength)\n",
    "    return CoLA.DataBatch(\n",
    "        inputs: textBatch, labels: example.isAcceptable.map { Tensor($0 ? 1 : 0) })\n",
    "}\n",
    "\n",
    "var cola = try CoLA(\n",
    "    exampleMap: exampleMapFn,\n",
    "    taskDirectoryURL: workspaceURL,\n",
    "    maxSequenceLength: maxSequenceLength,\n",
    "    batchSize: batchSize,\n",
    "    dropRemainder: true)\n",
    "\n",
    "print(\"Dataset acquired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT for the CoLA task!\n",
      "[Epoch 1]\n",
      "  Training loss: 0.7760596\n",
      "  Training loss: 0.5992831\n",
      "  Training loss: 0.6044539\n",
      "  Training loss: 0.5761066\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "execution_count": 4,
     "output_type": "error",
     "status": "error",
     "traceback": [
      "Current stack trace:",
      "\tframe #29: 0x00007f0fa7da54fd libjupyterInstalledPackages.so`AD__$s10TextModels23TransformerEncoderLayerV14callAsFunctiony10TensorFlow0I0VySfGAA0C5InputVySfGF__pullback_src_0_wrt_0_1 at TransformerBERT.swift:307:30 [opt]",
      "\tframe #30: 0x00007f0fa7daca2c libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels23TransformerEncoderLayerV14callAsFunctiony10TensorFlow0I0VySfGAA0C5InputVySfGF__pullback_src_0_wrt_0_1 at <compiler-generated>:0 [opt]",
      "\tframe #31: 0x00007f0fa7d4415f libjupyterInstalledPackages.so`AD__$s10TextModels4BERTV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at BERT.swift:340:60 [opt]",
      "\tframe #32: 0x00007f0fa7d4d78f libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels4BERTV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at <compiler-generated>:0 [opt]",
      "\tframe #33: 0x00007f0fa7d5e3cc libjupyterInstalledPackages.so`partial apply for thunk for @escaping @callee_guaranteed (@guaranteed Tensor<Float>) -> (@out BERT.TangentVector) [inlined] reabstraction thunk helper from @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@out TextModels.BERT.TangentVector) to @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@owned TextModels.BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #34: 0x00007f0fa7d5e3c9 libjupyterInstalledPackages.so`partial apply for thunk for @escaping @callee_guaranteed (@guaranteed Tensor<Float>) -> (@out BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #35: 0x00007f0fa7d5c335 libjupyterInstalledPackages.so`AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 [inlined] reabstraction thunk helper from @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@owned TextModels.BERT.TangentVector) to @escaping @callee_guaranteed (@guaranteed TensorFlow.Tensor<Swift.Float>) -> (@out TextModels.BERT.TangentVector) at <compiler-generated>:0 [opt]",
      "\tframe #36: 0x00007f0fa7d5c326 libjupyterInstalledPackages.so`AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at BERTClassifier.swift:40 [opt]",
      "\tframe #37: 0x00007f0fa7d5e486 libjupyterInstalledPackages.so`partial apply for AD__$s10TextModels14BERTClassifierV14callAsFunctiony10TensorFlow0G0VySfG12ModelSupport0A5BatchVF__pullback_src_0_wrt_1 at <compiler-generated>:0 [opt]",
      "\tframe #41: 0x00007f0fabe99c05 $__lldb_expr42`partial apply for AD__$s14__lldb_expr_4110TensorFlow0C0VySfG10TextModels14BERTClassifierVcfU0___pullback_src_0_wrt_0 [inlined]  at <Cell 4>:24",
      "\tframe #50: 0x00007f0fabe94f8f $__lldb_expr42`main at <Cell 4>:23:33"
     ]
    }
   ],
   "source": [
    "var optimizer = WeightDecayedAdam(\n",
    "    for: bertClassifier,\n",
    "    learningRate: LinearlyDecayedParameter(\n",
    "        baseParameter: LinearlyWarmedUpParameter(\n",
    "            baseParameter: FixedParameter<Float>(2e-5),\n",
    "            warmUpStepCount: 10,\n",
    "            warmUpOffset: 0),\n",
    "        slope: -5e-7,  // The LR decays linearly to zero in 100 steps.\n",
    "        startStep: 10),\n",
    "    weightDecayRate: 0.01,\n",
    "    maxGradientGlobalNorm: 1)\n",
    "\n",
    "print(\"Training BERT for the CoLA task!\")\n",
    "for epoch in 1...3 {\n",
    "    print(\"[Epoch \\(epoch)]\")\n",
    "    Context.local.learningPhase = .training\n",
    "    var trainingLossSum: Float = 0\n",
    "    var trainingBatchCount = 0\n",
    "    var trainingDataIterator = cola.trainDataIterator\n",
    "\n",
    "    while let batch = withDevice(.cpu, perform: { trainingDataIterator.next() }) {\n",
    "        let (documents, labels) = (batch.inputs, Tensor<Float>(batch.labels!))\n",
    "        let (loss, gradients) = valueWithGradient(at: bertClassifier) { model -> Tensor<Float> in\n",
    "            let logits = model(documents)\n",
    "            return sigmoidCrossEntropy(\n",
    "                logits: logits.squeezingShape(at: -1),\n",
    "                labels: labels,\n",
    "                reduction: { $0.mean() })\n",
    "        }\n",
    "\n",
    "        trainingLossSum += loss.scalarized()\n",
    "        trainingBatchCount += 1\n",
    "        optimizer.update(&bertClassifier, along: gradients)\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "              Training loss: \\(trainingLossSum / Float(trainingBatchCount))\n",
    "            \"\"\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    Context.local.learningPhase = .inference\n",
    "    var devLossSum: Float = 0\n",
    "    var devBatchCount = 0\n",
    "    var devDataIterator = cola.devDataIterator\n",
    "    var devPredictedLabels = [Bool]()\n",
    "    var devGroundTruth = [Bool]()\n",
    "    while let batch = withDevice(.cpu, perform: { devDataIterator.next() }) {\n",
    "        let (documents, labels) = (batch.inputs, batch.labels!)\n",
    "        let logits = bertClassifier(documents)\n",
    "        let loss = sigmoidCrossEntropy(\n",
    "            logits: logits.squeezingShape(at: -1),\n",
    "            labels: Tensor<Float>(labels),\n",
    "            reduction: { $0.mean() }\n",
    "        )\n",
    "        devLossSum += loss.scalarized()\n",
    "        devBatchCount += 1\n",
    "\n",
    "        let predictedLabels = sigmoid(logits.squeezingShape(at: -1)) .>= 0.5\n",
    "        devPredictedLabels.append(contentsOf: predictedLabels.scalars)\n",
    "        devGroundTruth.append(contentsOf: labels.scalars.map { $0 == 1 })\n",
    "    }\n",
    "\n",
    "    let mcc = matthewsCorrelationCoefficient(\n",
    "        predictions: devPredictedLabels,\n",
    "        groundTruth: devGroundTruth)\n",
    "\n",
    "    print(\n",
    "        \"\"\"\n",
    "          MCC: \\(mcc)\n",
    "          Eval loss: \\(devLossSum / Float(devBatchCount))\n",
    "        \"\"\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
